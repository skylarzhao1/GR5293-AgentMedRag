{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTwpDNBnorUT"
      },
      "source": [
        "# ColBERTv2: Indexing & Search Notebook\n",
        "\n",
        "If you're working in Google Colab, we recommend selecting \"GPU\" as your hardware accelerator in the runtime settings.\n",
        "\n",
        "First, we'll import the relevant classes. Note that `Indexer` and `Searcher` are the key actors here. Next, we'll download the necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_YBBPTo5AZ",
        "outputId": "c0c7f2b2-0b1c-4e84-9c7b-d0b075583914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: cannot change to 'ColBERT/': No such file or directory\n",
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 2846, done.\u001b[K\n",
            "remote: Counting objects: 100% (1208/1208), done.\u001b[K\n",
            "remote: Compressing objects: 100% (353/353), done.\u001b[K\n",
            "remote: Total 2846 (delta 985), reused 855 (delta 855), pack-reused 1638 (from 4)\u001b[K\n",
            "Receiving objects: 100% (2846/2846), 2.07 MiB | 10.88 MiB/s, done.\n",
            "Resolving deltas: 100% (1792/1792), done.\n"
          ]
        }
      ],
      "source": [
        "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
        "import sys; sys.path.insert(0, 'ColBERT/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmBi2UT5pxb3",
        "outputId": "b102d40f-2738-45d0-8a22-8612222385d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Obtaining file:///content/ColBERT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray (from colbert-ai==0.2.20)\n",
            "  Downloading bitarray-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
            "Collecting datasets (from colbert-ai==0.2.20)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.1.0)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (3.1.44)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.20)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.20)\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from colbert-ai==0.2.20) (4.51.3)\n",
            "Collecting ujson (from colbert-ai==0.2.20)\n",
            "  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "INFO: pip is looking at multiple versions of colbert-ai[faiss-gpu,torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu>=1.7.0; extra == \"faiss-gpu\" (from colbert-ai[faiss-gpu,torch]) (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu>=1.7.0; extra == \"faiss-gpu\"\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try: # When on google Colab, let's install all dependencies with pip.\n",
        "    import google.colab\n",
        "    !pip install -U pip\n",
        "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
        "except Exception:\n",
        "  import sys; sys.path.insert(0, 'ColBERT/')\n",
        "  try:\n",
        "    from colbert import Indexer, Searcher\n",
        "  except Exception:\n",
        "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
        "    assert False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0jxbVar4kln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5342f0e9-8304-4d17-e7cb-eaadd6ff7a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ujson\n",
            "  Using cached ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Installing collected packages: ujson\n",
            "Successfully installed ujson-5.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ujson\n",
        "\n",
        "import colbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg9A-dtp1nB"
      },
      "outputs": [],
      "source": [
        "from colbert import Indexer, Searcher\n",
        "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
        "from colbert.data import Queries, Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLWjmlkVq9r0"
      },
      "source": [
        "We will use the dev set of the **LoTTE benchmark** we recently introduced in the ColBERTv2 paper. We'll download it from HuggingFace datasets. The dev and test sets contain several domain-specific corpora, and we'll use the smallest dev set corpus, namely lifestyle:dev.\n",
        "\n",
        "For the purposes of a quick demo, we will only run the `Indexer` on the first 10,000 passages. As we do this, let's also remove the queries whose relevant passages are all outside this small set of passages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rF7lv8jvq-ut"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = 'lifestyle'\n",
        "datasplit = 'dev'\n",
        "\n",
        "collection_dataset = load_dataset(\"colbertv2/lotte_passages\", dataset)\n",
        "collection = [x['text'] for x in collection_dataset[datasplit + '_collection']]\n",
        "\n",
        "queries_dataset = load_dataset(\"colbertv2/lotte\", dataset)\n",
        "queries = [x['query'] for x in queries_dataset['search_' + datasplit]]\n",
        "\n",
        "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXKC1oeUsnhk"
      },
      "source": [
        "This loaded 417 queries and 269k passages. Let's inspect one query and one passage to verify we have done so correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQFUHYTZs0aa"
      },
      "outputs": [],
      "source": [
        "print(queries[24])\n",
        "print()\n",
        "print(collection[19929])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJdAAbDu7PZ"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "For an efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKAdVN5MvDKD"
      },
      "outputs": [],
      "source": [
        "nbits = 2   # encode each dimension with 2 bits\n",
        "doc_maxlen = 300 # truncate passages at 300 tokens\n",
        "max_id = 10000\n",
        "\n",
        "index_name = f'{dataset}.{datasplit}.{nbits}bits'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsImrM-rAUzi"
      },
      "source": [
        "To save space and time, we will only run the `Indexer` on the first 10,000 passages. To do so, we will filter out queries that do not contain passages with ids less than 10,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDkbZCvY_f7x"
      },
      "outputs": [],
      "source": [
        "answer_pids = [x['answers']['answer_pids'] for x in queries_dataset['search_' + datasplit]]\n",
        "filtered_queries = [q for q, apids in zip(queries, answer_pids) if any(x < max_id for x in apids)]\n",
        "\n",
        "f'Filtered down to {len(filtered_queries)} queries'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orKfQRmQv46u"
      },
      "source": [
        "Now run the `Indexer` on the collection subset. Assuming the use of only one GPU, this cell should take about six minutes to finish running."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRiOnzxtwI0j"
      },
      "outputs": [],
      "source": [
        "checkpoint = 'colbert-ir/colbertv2.0'\n",
        "\n",
        "with Run().context(RunConfig(nranks=1, experiment='notebook')):  # nranks specifies the number of GPUs to use\n",
        "    config = ColBERTConfig(doc_maxlen=doc_maxlen, nbits=nbits, kmeans_niters=4) # kmeans_niters specifies the number of iterations of k-means clustering; 4 is a good and fast default.\n",
        "                                                                                # Consider larger numbers for small datasets.\n",
        "\n",
        "    indexer = Indexer(checkpoint=checkpoint, config=config)\n",
        "    indexer.index(name=index_name, collection=collection[:max_id], overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTbP2LS1xHVZ"
      },
      "outputs": [],
      "source": [
        "indexer.get_index() # You can get the absolute path of the index, if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY6_D523yBFB"
      },
      "source": [
        "## Search\n",
        "\n",
        "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
        "\n",
        "We can use the `queries` set we loaded earlier — or you can supply your own questions. Feel free to get creative! But keep in mind this set of ~300k lifestyle passages can only answer a small, focused set of questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3x_FnVnyB0n"
      },
      "outputs": [],
      "source": [
        "# To create the searcher using its relative name (i.e., not a full path), set\n",
        "# experiment=value_used_for_indexing in the RunConfig.\n",
        "with Run().context(RunConfig(experiment='notebook')):\n",
        "    searcher = Searcher(index=index_name, collection=collection)\n",
        "\n",
        "\n",
        "# If you want to customize the search latency--quality tradeoff, you can also supply a\n",
        "# config=ColBERTConfig(ncells=.., centroid_score_threshold=.., ndocs=..) argument.\n",
        "# The default settings with k <= 10 (1, 0.5, 256) gives the fastest search,\n",
        "# but you can gain more extensive search by setting larger values of k or\n",
        "# manually specifying more conservative ColBERTConfig settings (e.g. (4, 0.4, 4096))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JYA0N22yIeS"
      },
      "outputs": [],
      "source": [
        "query = filtered_queries[13] # try with an in-range query or supply your own\n",
        "print(f\"#> {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}