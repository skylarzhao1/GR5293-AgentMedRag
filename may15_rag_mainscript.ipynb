{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AELZHL_EseiF"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following block to install required libraries\n",
    "# \"\"\"\n",
    "# !pip install langchain chromadb sentence-transformers\n",
    "# !pip install  openai tiktoken\n",
    "# !pip install jq\n",
    "# !pip install faiss\n",
    "# !pip install pymilvus\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj_vDYRdWpiT",
    "outputId": "a9e136f4-3b8e-4ffd-fa10-e8eb98ed5756"
   },
   "outputs": [],
   "source": [
    "!pip install ujson\n",
    "import colbert\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhRylV-cEr7S",
    "outputId": "5d12aab5-2669-4cd1-b5cb-007cbe0eb0e9"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index-core transformers torch\n",
    "!pip install llama-index-postprocessor-colbert-rerank\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkRGq6xQANRj"
   },
   "outputs": [],
   "source": [
    "!pip install -qU bitsandbytes transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZYc6m2xAO1N"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0hMy18ruhrt",
    "outputId": "04370858-ac4f-4050-e451-493cd1db6cb3"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHouMxxBseiG"
   },
   "source": [
    "* Load the PubMed articles from the JSON file. To prepare the JSON file, please refer to the script `download_pubmed.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHWruWXiseiG",
    "outputId": "301150c9-6672-4894-c655-ef147e31d4bc"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community\n",
    "!pip install jq\n",
    "#load public article from json\n",
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # Define the metadata extraction function.\n",
    "    metadata[\"year\"] = record.get(\"pub_date\").get('year')\n",
    "    metadata[\"month\"] = record.get(\"pub_date\").get('month')\n",
    "    metadata[\"day\"] = record.get(\"pub_date\").get('day')\n",
    "    metadata[\"title\"] = record.get(\"article_title\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='/content/drive/MyDrive/finalproject/pubmed_article.json',\n",
    "    jq_schema='.[]',\n",
    "    content_key='article_abstract',\n",
    "    metadata_func=metadata_func)\n",
    "data = loader.load()\n",
    "print(f\"{len(data)} pubmed articles are loaded!\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVAZ8sKtW6AA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76voz0KMseiH"
   },
   "source": [
    "- Chunk abstracts into small text passages for efficient retrieval and LLM context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaag0-jRseiH",
    "outputId": "1c7b7f4f-a3c9-4fd0-c73c-524a72c60f47"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "#只对page content 做chunking\n",
    "from langchain.text_splitter import TokenTextSplitter,CharacterTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=64)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"{len(data)} pubmed articles are converted to {len(chunks)} text fragments!\")\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMZeOqweseiI"
   },
   "outputs": [],
   "source": [
    "# Option a: using all-mpnet from SentenceTransformer\n",
    "# from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# # Option b: using e5-large-unspupervised from huggingface\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# modelPath = \"intfloat/e5-large-unsupervised\"\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#   model_name = modelPath,\n",
    "#   model_kwargs = {'device':'cuda'},\n",
    "#   encode_kwargs={'normalize_embeddings':False}\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "# embeddings = SentenceTransformerEmbeddings(\n",
    "#     model_name=\"intfloat/e5-large-v2\",\n",
    "#     model_kwargs={\"device\": \"cuda\"}\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GVRyLeI2Kvp",
    "outputId": "0cfc2bc1-d78a-4abe-8669-232c94da57ba"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WVUzXU_sseiI",
    "outputId": "501f6bdd-e9f2-4e0f-ccbf-144a0350843a"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Option a: Using chroma database\n",
    "from langchain.vectorstores import Chroma\n",
    "# db = Chroma.from_documents(chunks, embeddings)\n",
    "# 用正确的 1024 维模型加载\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "# Option b: Using Milvus database\n",
    "# To run the following code, you should have a milvus instance up and running\n",
    "# Follow the instructions in the following the link: https://milvus.io/docs/install_standalone-docker.md\n",
    "from langchain.vectorstores import Milvus\n",
    "db = Milvus.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"},\n",
    ")\n",
    "'''\n",
    "\n",
    "# # Using faiss index\n",
    "# from langchain.vectorstores import FAISS\n",
    "# db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "3307aca423f441249414a402b3cebf9d",
      "b13281471051409daa88a6b7879df371",
      "5507bdb230f04246b5ed7244ff9b14b5",
      "83ee3ecacbcb4bdaa76f9c07c0ae9a9e",
      "3f920d5d9c8341158df7776c3591ed77",
      "53b3a5433b9d44a984973c720ea3ea87",
      "349c72e9f2904efbad40caddb24ee728",
      "f7e678c63255426caa83f64f3c0b979f",
      "6c1bfba0b76d4bbc98a7892a6cf3daba",
      "7cd65fa02137447b8882cf31ab01dd72",
      "f6a8cdd167f348739566d4970622c0a4",
      "226460fc41404c6bacc079a526422cb8",
      "fa1223c646fb4423a3fb58aa2e164f8c",
      "fd5b09da2df741c99bf616baa7616561",
      "d4b61da6a6f74f68b673ea62f805ad4a",
      "9c795492e3204e809a1c1d62aa079cd1",
      "eb615b86fca14869ad75fb2695a12e58",
      "a04a847c622544de97a7f3d7f2d3edf4",
      "89e47c9151424bdb91bd66b005613bad",
      "181969fc5d5c4e66ac4e62d3f39d492a",
      "bc5c049cfa5d4110b3a48298b426062d",
      "3822aea8c5db4129a34e9ae386005a35",
      "d8cbd3afabca409992d952bf9f31cb1b",
      "46b9f72374b44ae6baf428e18b581d9b",
      "06192c05c9e8430f83f5283f835eddce",
      "804d3505ef6a4a2480d0fd39588185f1",
      "193f483cd1e541469044c3b306f91111",
      "c45e11df0db14bed8298dd32ea78cf77"
     ]
    },
    "id": "AY9m6suKy0W4",
    "outputId": "e61957e5-4276-4e55-d747-320dcc6337d8"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# First uninstall existing versions\n",
    "# !pip install -U bitsandbytes\n",
    "\n",
    "# # Install with CUDA 11.x support (works for most systems)\n",
    "# !pip install -U bitsandbytes>=0.41.1\n",
    "# !pip install -U transformers accelerate\n",
    "\n",
    "\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "# # Verify installation\n",
    "# import bitsandbytes\n",
    "# print(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# # 4-bit quantization config\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_quant_type=\"nf4\"\n",
    "# )\n",
    "\n",
    "# Load model (Mistral-7B)\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    temperature=0.1,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# LangChain wrapper\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00XdR-8VyzQ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7ySDK99mhEh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eQRzlviGseiI"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "\n",
    "# PROMPT 1\n",
    "PROMPT_TEMPLATE_1 = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "You are allowed to rephrase the answer based on the context.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT1 = PromptTemplate.from_template(PROMPT_TEMPLATE_1)\n",
    "\n",
    "# # PROMPT 2\n",
    "# PROMPT_TEMPLATE_2=\"Your are a medical assistant for question-answering tasks. Answer the Question using the provided Contex only. Your answer should be in your own words and be no longer than 128 words. \\n\\n Context: {context} \\n\\n Question: {question} \\n\\n Answer:\"\n",
    "# PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "# 1. 修正Prompt\n",
    "PROMPT_TEMPLATE_2 = \"\"\"Answer based ONLY on:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Concise medical answer (max 100 words):\"\"\"\n",
    "PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "\n",
    "# # 3. 执行查询\n",
    "# start_time = time.time()\n",
    "# result = qa_chain.invoke({\"query\": \"Alzheimer's treatments\"})\n",
    "# print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "# print(result[\"result\"])\n",
    "# PROMPT 3\n",
    "# from langchain import hub\n",
    "# PROMPT3 = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
    "\n",
    "# # RAG pipeline\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm,\n",
    "#     retriever=db.as_retriever(k=4),\n",
    "#     chain_type_kwargs={\"prompt\": PROMPT2},\n",
    "#     return_source_documents=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4OyT1eX5seiI"
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# # query = \"What are the safest cryopreservation methods?\"\n",
    "# query=\"What are the recent advancements in the treatment of Alzheimer’s disease?\"\n",
    "# result = qa_chain({\"query\": query})\n",
    "# print(f\"\\n--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT2GflVwseiJ"
   },
   "outputs": [],
   "source": [
    "# # Define the langchain pipeline for llm only\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# PROMPT_TEMPLATE =\"\"\"Answer the given Question only. Your answer should be in your own words and be no longer than 100 words. \\n\\n Question: {question} \\n\\n\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "# PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "# llm_chain = PROMPT | llm\n",
    "# start_time = time.time()\n",
    "# result = llm_chain.invoke({\"question\": query})\n",
    "# print(f\"\\n--- {time.time() - start_time} seconds ---\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPpo9VCtErN1"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# #Indexing\n",
    "\n",
    "\n",
    "# nbits = 2   # encode each dimension with 2 bits\n",
    "# doc_maxlen = 128 # truncate passages at 300 tokens\n",
    "# max_id = 20\n",
    "# checkpoint = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "\n",
    "# collection = [chunk.page_content for chunk in chunks[:max_id]]  # 限制了 max_id=2400，所以截取前 2400 条\n",
    "\n",
    "\n",
    "# index_name = 'pubmed_index_small'\n",
    "\n",
    "# with Run().context(RunConfig(nranks=1, experiment=\"/content/drive/MyDrive/finalproject/exp_pubmed\")):\n",
    "\n",
    "#     config = ColBERTConfig(doc_maxlen=128, nbits=2, kmeans_niters=2)\n",
    "#     indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "#     indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1X3G6lVoILFZ",
    "outputId": "eff38454-8487-4e6a-e3f9-afcb7365ac8d"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMMvt5Q0ILHx"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  #\n",
    "def rerank_bge(query, documents, top_k):\n",
    "    scores = []\n",
    "    for doc in documents:\n",
    "        inputs = tokenizer(\n",
    "            query, doc.page_content,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            score = logits.squeeze().item()  #\n",
    "\n",
    "        scores.append((doc, score))\n",
    "\n",
    "    #sort by scores(descending)\n",
    "\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "     # Deduplicate by title (for this query)\n",
    "    seen_titles = set()\n",
    "    reranked_unique = []\n",
    "    # reranked = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    # for i, (doc, score) in enumerate(reranked):\n",
    "    #     print(f\"Reranked #{i+1} | Score: {score:.4f}\")\n",
    "    #     print(f\"Title: {doc.metadata.get('title')}\")\n",
    "    #     print(f\"Content: {doc.page_content[:300]}...\\n\")\n",
    "    # return [doc for doc, score in reranked]\n",
    "\n",
    "    for doc, score in sorted_scores:\n",
    "        title = doc.metadata.get(\"title\", None)\n",
    "\n",
    "        if title and title not in seen_titles:\n",
    "            seen_titles.add(title)\n",
    "            reranked_unique.append((doc, score))\n",
    "\n",
    "        if len(reranked_unique) >= top_k:\n",
    "            break\n",
    "\n",
    "    # Print results\n",
    "    for i, (doc, score) in enumerate(reranked_unique):\n",
    "        print(f\"Reranked #{i+1} | Score: {score:.4f}\")\n",
    "        print(f\"Title: {doc.metadata.get('title')}\")\n",
    "        print(f\"Content: {doc.page_content[:300]}...\\n\")\n",
    "\n",
    "    return [doc for doc, score in reranked_unique]\n",
    "\n",
    "\n",
    "# query = \"What are the recent advancements in the treatment of Alzheimer’s disease?\"\n",
    "# retrieved_docs = db.as_retriever(k=8).get_relevant_documents(query)\n",
    "\n",
    "# # rerank\n",
    "# reranked_docs = rerank_bge(query, retrieved_docs, top_k=7)\n",
    "\n",
    "\n",
    "# print(f\"\\n Query: {query}\")\n",
    "# print(f\" Top {len(reranked_docs)} reranked results:\\n\")\n",
    "# for i, doc in enumerate(reranked_docs):\n",
    "#     print(f\"Reranked #{i+1}:\")\n",
    "#     print(f\"Title: {doc.metadata.get('title')}\")\n",
    "#     print(f\"Content: {doc.page_content[:300]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-tUQclT1_0Sf",
    "outputId": "23a63e71-cdd1-40c9-a383-e648598d75ba"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"./chroma_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Pp5D44RCBpK"
   },
   "outputs": [],
   "source": [
    "!pip install ragas\n",
    "\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import Faithfulness\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XseHTrsNM_6i",
    "outputId": "304e293c-3af2-4903-d36d-cb90d236e9a3"
   },
   "outputs": [],
   "source": [
    "import ragas\n",
    "import langchain_core\n",
    "import importlib.metadata  # Python 3.8+\n",
    "\n",
    "# Method 1: Using importlib (most reliable)\n",
    "print(f\"RAGAS version: {importlib.metadata.version('ragas')}\")\n",
    "print(f\"LangChain Core: {importlib.metadata.version('langchain-core')}\")\n",
    "print(f\"LangChain OpenAI: {importlib.metadata.version('langchain-openai')}\")\n",
    "\n",
    "# Method 2: Alternative for older Python\n",
    "try:\n",
    "    from pip._internal.operations import freeze\n",
    "    pkgs = freeze.freeze()\n",
    "    print(\"\\nAll installed packages:\")\n",
    "    for pkg in pkgs:\n",
    "        if 'ragas' in pkg.lower() or 'langchain' in pkg.lower():\n",
    "            print(pkg)\n",
    "except ImportError:\n",
    "    !pip list | grep -E 'ragas|langchain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b02649bb603e49019a740424ba37d3e2",
      "2783daf73f5941d7bb934fea73f2c8eb",
      "f5f8f4a20c024b0e9e6ea503748edbd5",
      "7b5bb8ec88e7485dac1c5eff990730b7",
      "b97ef636af954f5c9223060f841bbb6b",
      "ecc6f68025694428bf0fa1887c0fd264",
      "14974c70ccd5449f8337e978f1b1e9bd",
      "7c8e8e1ed2fb434f87699f4e118ef05e",
      "cb1e5ed2fc0e4060af34dc4446fb2080",
      "58f8d1f0bd804aca8a639a652cfa43ad",
      "d32c71ab7daa4c2ebab6003d8359f838"
     ]
    },
    "id": "sIOVPttweOsY",
    "outputId": "31f0de0b-ba4f-4c04-f10c-0c3d2e0a3c77"
   },
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "!pip install -U ragas langchain-openai datasets\n",
    "\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "queries = [\n",
    "   \"How does sonodynamic therapy (SDT) differ from conventional antibiotics in terms of combating multidrug-resistant bacterial infections?\",\n",
    "   \"Does zebrafish possess an ortholog or paralog of mammalian calprotectin that exhibits antimicrobial activity and can activate inflammation via Toll-like receptor 4?\",\n",
    "   \"What is the current understanding of gray matter alterations in patients with vestibular migraine?\",\n",
    "   \"How do nickel clusters improve the hydrogen evolution reaction (HER) and what potential applications does this have in renewable energy conversion?\",\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "  \" Sonodynamic therapy (SDT) leverages the generation of reactive oxygen species to inflict multifaceted damage on bacterial cells, reducing the likelihood of developing drug resistance, unlike conventional antibiotics.\",\n",
    "   \"No, zebrafish do not have an ortholog of either mammalian S100A8 or S100A9, and none of the identified zebrafish s100 proteins exhibited antimicrobial activity comparable to mammalian calprotectin. Additionally, none of the zebrafish proteins activated inflammation via Toll-like receptor 4, suggesting that similar proteins have not convergently evolved analogous functions.\",\n",
    "   \"The current understanding of gray matter alterations in patients with vestibular migraine remains lacking, despite the growing amount of neuroimaging data in recent decades.\",\n",
    "   \"clusters considerably improve the hydrogen evolution reaction (HER), indicating their promise for renewable energy conversion.\"\n",
    "]\n",
    "\n",
    "\n",
    "all_contexts = []\n",
    "for query in queries:\n",
    "    retrieved_docs = db.as_retriever(k=15).get_relevant_documents(query)  # \n",
    "    reranked_docs = rerank_bge(query, retrieved_docs, top_k=4)          # \n",
    "    all_contexts.append([doc.page_content for doc in reranked_docs])\n",
    "  \n",
    "\n",
    "# 2. Initialize wrappers\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# 3. Define your test data (modified for vestibular migraine example)\n",
    "test_data = {\n",
    "    \"question\": queries,\n",
    "    # \"answer\": [\"n/a\"] * len(queries),\n",
    "    \"contexts\": all_contexts,\n",
    "   \"ground_truth\": reference_answers\n",
    "}\n",
    "\n",
    "# 4. Convert to Dataset format\n",
    "dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# 5. Define metrics (using newer RAGAS syntax)\n",
    "from ragas.metrics import  context_precision\n",
    "\n",
    "# # Correct way to access results in RAGAS ≥0.2.0\n",
    "# context_precision_score = results['context_precision'][0]  # Get first (and only) score\n",
    "# print(f\"Context Precision Score: {context_precision_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n evaluating retrieval quality...\")\n",
    "results = evaluate(\n",
    "    dataset,\n",
    "    metrics=[context_precision],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n Evaluation Results:\")\n",
    "print(\"=\"*50)\n",
    "for i, (query, score) in enumerate(zip(queries, results['context_precision'])):\n",
    "    print(f\"\\nQuestions {i+1}: {query}...\")\n",
    "    print(f\"Context Precision: {score:.2f}/1.0\")\n",
    "    print(f\" Number of relevant documents:: {len(all_contexts[i])}\")\n",
    "    print(\" Most relevant document summaries:\")\n",
    "    for j, ctx in enumerate(all_contexts[i][:2]):  \n",
    "        print(f\"  {j+1}. {ctx[:80]}...\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8ee66378ac8c4baf89f499e8a190aee5",
      "cd5a4163264840ddb9d61ea14202a61a",
      "ba6aabc01fb1485e93aefe8f1a2bb635",
      "9b7a880ac9284508b760c91ea9fcd5d1",
      "c8f3539d093e477c8e8a1a4339bb46b0",
      "ce7e495ee5ad4cdba104fa9bb5591d6a",
      "336a58577ca045ef9c1e16ec00ff9137",
      "83a9a4f0d861492bb6edb3068c706d15",
      "06d574e59bb24e56976b85708771acb8",
      "42ea1aaa85c2461b9b2b45af8367cbc1",
      "969ac9de81ce4d2d9fe5240fa39d3ef3"
     ]
    },
    "id": "MdwHFywdeOze",
    "outputId": "1181ec31-5a1f-4ae1-ab52-1a18464ffc0c"
   },
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "!pip install -U ragas langchain-openai datasets\n",
    "\n",
    "import os\n",
    "from ragas.metrics import context_recall\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "queries = [\n",
    "   \"How does sonodynamic therapy (SDT) differ from conventional antibiotics in terms of combating multidrug-resistant bacterial infections?\",\n",
    "   \"Does zebrafish possess an ortholog or paralog of mammalian calprotectin that exhibits antimicrobial activity and can activate inflammation via Toll-like receptor 4?\",\n",
    "   \"What is the current understanding of gray matter alterations in patients with vestibular migraine?\",\n",
    "   \"How do nickel clusters improve the hydrogen evolution reaction (HER) and what potential applications does this have in renewable energy conversion?\",\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "  \" Sonodynamic therapy (SDT) leverages the generation of reactive oxygen species to inflict multifaceted damage on bacterial cells, reducing the likelihood of developing drug resistance, unlike conventional antibiotics.\",\n",
    "   \"No, zebrafish do not have an ortholog of either mammalian S100A8 or S100A9, and none of the identified zebrafish s100 proteins exhibited antimicrobial activity comparable to mammalian calprotectin. Additionally, none of the zebrafish proteins activated inflammation via Toll-like receptor 4, suggesting that similar proteins have not convergently evolved analogous functions.\",\n",
    "   \"The current understanding of gray matter alterations in patients with vestibular migraine remains lacking, despite the growing amount of neuroimaging data in recent decades.\",\n",
    "   \"clusters considerably improve the hydrogen evolution reaction (HER), indicating their promise for renewable energy conversion.\"\n",
    "]\n",
    "\n",
    "\n",
    "all_contexts = []\n",
    "for query in queries:\n",
    "    retrieved_docs = db.as_retriever(k=30).get_relevant_documents(query)  # 替换为您的检索系统\n",
    "    reranked_docs = rerank_bge(query, retrieved_docs, top_k=10)          # 替换为您的重排序\n",
    "    all_contexts.append([doc.page_content for doc in reranked_docs])\n",
    "   \n",
    "\n",
    "# 2. Initialize wrappers\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# 3. Define your test data (modified for vestibular migraine example)\n",
    "test_data = {\n",
    "    \"question\": queries,\n",
    "    # \"answer\": [\"n/a\"] * len(queries),\n",
    "    \"contexts\": all_contexts,\n",
    "   \"ground_truth\": reference_answers\n",
    "}\n",
    "\n",
    "# 4. Convert to Dataset format\n",
    "dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# 5. Define metrics (using newer RAGAS syntax)\n",
    "from ragas.metrics import  context_recall\n",
    "\n",
    "# # Correct way to access results in RAGAS ≥0.2.0\n",
    "# context_precision_score = results['context_precision'][0]  # Get first (and only) score\n",
    "# print(f\"Context Precision Score: {context_precision_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n 正在评估检索质量...\")\n",
    "results = evaluate(\n",
    "    dataset,\n",
    "    metrics=[context_recall],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n Evaluating Results :\")\n",
    "print(\"=\"*50)\n",
    "for i, (query, score) in enumerate(zip(queries, results['context_recall'])):\n",
    "    print(f\"\\nQuestion {i+1}: {query}...\")\n",
    "    print(f\"• Context recall: {score:.2f}/1.0\")\n",
    "    print(f\"• number of relevant documents: {len(all_contexts[i])}\")\n",
    "    print(\"• Most relevant document summaries :\")\n",
    "    for j, ctx in enumerate(all_contexts[i][:2]):  # 显示前两个文档\n",
    "        print(f\"  {j+1}. {ctx[:380]}...\")\n",
    "print(\"=\"*50)\n",
    "# print(\"\\n💡 评分说明: 1.0表示完美匹配，0.5以下需改进检索系统\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b42c3c9e587d4bac86af0a271d5d5f1c",
      "80eb6dee021148239e475145bb79cfb3",
      "094f4e900fd54553ae0e9e96fdcb4051",
      "6c386347986741d887edc0fa729c9c53",
      "a532694ded2742078da7816f4ac957b8",
      "0d1bbe9722a44a5494988e7206ec832b",
      "bbfc6305173844d7bbe85a2b6798a850",
      "63c9b107da7e457dabecc4f5a1ba09ef",
      "b2db115cb72d457382680ed765a9ecca",
      "2d0da4900e1d41739980dd6a505fbfde",
      "030c0cea9df441f5a624a39d6c7a694e"
     ]
    },
    "id": "9NAfeWFMeO9F",
    "outputId": "68c7714f-4f47-45f0-d11f-f763e4c66695"
   },
   "outputs": [],
   "source": [
    "# Installation (if needed)\n",
    "!pip install -U ragas langchain-openai datasets\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness  # 主要修改点\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "generated_response=[]\n",
    "queries = [\n",
    "   \"How does sonodynamic therapy (SDT) differ from conventional antibiotics in terms of combating multidrug-resistant bacterial infections?\",\n",
    "   \"Does zebrafish possess an ortholog or paralog of mammalian calprotectin that exhibits antimicrobial activity and can activate inflammation via Toll-like receptor 4?\",\n",
    "   \"What is the current understanding of gray matter alterations in patients with vestibular migraine?\",\n",
    "   \"How do nickel clusters improve the hydrogen evolution reaction (HER) and what potential applications does this have in renewable energy conversion?\",\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "  \" Sonodynamic therapy (SDT) leverages the generation of reactive oxygen species to inflict multifaceted damage on bacterial cells, reducing the likelihood of developing drug resistance, unlike conventional antibiotics.\",\n",
    "   \"No, zebrafish do not have an ortholog of either mammalian S100A8 or S100A9, and none of the identified zebrafish s100 proteins exhibited antimicrobial activity comparable to mammalian calprotectin. Additionally, none of the zebrafish proteins activated inflammation via Toll-like receptor 4, suggesting that similar proteins have not convergently evolved analogous functions.\",\n",
    "   \"The current understanding of gray matter alterations in patients with vestibular migraine remains lacking, despite the growing amount of neuroimaging data in recent decades.\",\n",
    "   \"clusters considerably improve the hydrogen evolution reaction (HER), indicating their promise for renewable energy conversion.\"\n",
    "]\n",
    "\n",
    "\n",
    "all_contexts = []\n",
    "for query in queries:\n",
    "    retrieved_docs = db.as_retriever(k=15).get_relevant_documents(query)  # 替换为您的检索系统\n",
    "    reranked_docs = rerank_bge(query, retrieved_docs, top_k=4)          # 替换为您的重排序\n",
    "    all_contexts.append([doc.page_content for doc in reranked_docs])\n",
    "    # # 在您的代码中，在构建 all_contexts 后添加：\n",
    "    # print(\"\\n=== all_contexts 结构分析 ===\")\n",
    "    # print(f\"总查询数量: {len(all_contexts)}\")\n",
    "    # for i, contexts in enumerate(all_contexts):\n",
    "    #     print(f\"\\n问题 {i+1} 的文档内容:\")\n",
    "    #     print(f\"共检索到 {len(contexts)} 个文档\")\n",
    "    #     for j, content in enumerate(contexts):\n",
    "    #         print(f\"[文档 {j+1}] 长度:{len(content)} 字符\")\n",
    "    #         print(f\"内容预览: {content[:100]}...\")  # 只打印前100个字符\n",
    "\n",
    "    # Step 2: 拼接 prompt\n",
    "    context_text = \"\\n\\n\".join(contexts)\n",
    "    prompt = f\"\"\"Answer based ONLY on the context below.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Step 3: LLM 推理（RAG 生成的回答）\n",
    "    rag_response = evaluator_llm.generate(prompt)  # 如果你用的是 ChatOpenAI 包装过的 LangchainLLMWrapper\n",
    "    generated_responses.append(rag_response.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Initialize wrappers\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# 3. Define your test data (modified for vestibular migraine example)\n",
    "test_data = {\n",
    "    \"question\": queries,\n",
    "    # \"answer\": [\"n/a\"] * len(queries),\n",
    "\n",
    "    \"contexts\": all_contexts,\n",
    "    \"response\":\n",
    "}\n",
    "\n",
    "# 4. Convert to Dataset format\n",
    "dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# 5. Define metrics (using newer RAGAS syntax)\n",
    "from ragas.metrics import  context_recall\n",
    "\n",
    "# # Correct way to access results in RAGAS ≥0.2.0\n",
    "# context_precision_score = results['context_precision'][0]  # Get first (and only) score\n",
    "# print(f\"Context Precision Score: {context_precision_score:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n evaluating generating quality...\")\n",
    "results = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=evaluator_llm\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nevaluating results:\")\n",
    "print(\"=\"*50)\n",
    "for i, (query, score) in enumerate(zip(queries, results['faithfulness'])):\n",
    "    print(f\"\\nquestion {i+1}: {query[:60]}...\")\n",
    "    print(f\"• Faithfulness: {score:.2f}/1.0\")\n",
    "    print(f\"• the number of documents used: {len(all_contexts[i])}\")\n",
    "    print(\"• document content verification:\")\n",
    "    for j, ctx in enumerate(all_contexts[i][:2]):\n",
    "        print(f\"  {j+1}. {ctx[:80]}...\")\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyBU6P9cePCc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mtVAneh0RjQX",
    "outputId": "335c8b56-af79-410d-8261-6ab7daedc479"
   },
   "outputs": [],
   "source": [
    "#  Step 0: Installation (skip if already installed)\n",
    "!pip install -U ragas langchain-openai datasets\n",
    "\n",
    "#  Step 1: Imports & API Key\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import faithfulness  # Optional: add more metrics later\n",
    "\n",
    "queries = [\n",
    "   \"How does sonodynamic therapy (SDT) differ from conventional antibiotics in terms of combating multidrug-resistant bacterial infections?\",\n",
    "   \"Does zebrafish possess an ortholog or paralog of mammalian calprotectin that exhibits antimicrobial activity and can activate inflammation via Toll-like receptor 4?\",\n",
    "   \"What is the current understanding of gray matter alterations in patients with vestibular migraine?\",\n",
    "   \"How do nickel clusters improve the hydrogen evolution reaction (HER) and what potential applications does this have in renewable energy conversion?\",\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "  \" Sonodynamic therapy (SDT) leverages the generation of reactive oxygen species to inflict multifaceted damage on bacterial cells, reducing the likelihood of developing drug resistance, unlike conventional antibiotics.\",\n",
    "   \"No, zebrafish do not have an ortholog of either mammalian S100A8 or S100A9, and none of the identified zebrafish s100 proteins exhibited antimicrobial activity comparable to mammalian calprotectin. Additionally, none of the zebrafish proteins activated inflammation via Toll-like receptor 4, suggesting that similar proteins have not convergently evolved analogous functions.\",\n",
    "   \"The current understanding of gray matter alterations in patients with vestibular migraine remains lacking, despite the growing amount of neuroimaging data in recent decades.\",\n",
    "   \"clusters considerably improve the hydrogen evolution reaction (HER), indicating their promise for renewable energy conversion.\"\n",
    "]\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# PROMPT_TEMPLATE_2 = \"\"\"Answer based ONLY on:\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "# Concise medical answer (max 100 words):\"\"\"\n",
    "# PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Initialize LLM and embedding wrappers for RAGAS\n",
    "llm_wrapper = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "embedding_wrapper = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# Step 4: Retrieval + Generation (Replace with your real retrieval + reranker)\n",
    "all_contexts = []\n",
    "generated_responses = []\n",
    "\n",
    "for query in queries:\n",
    "    #  Replace these two lines with your real retrieval system (e.g., Chroma, FAISS)\n",
    "    retrieved_docs = db.as_retriever(k=15).get_relevant_documents(query)\n",
    "    reranked_docs = rerank_bge(query, retrieved_docs, top_k=4)\n",
    "\n",
    "    #  Extract context text\n",
    "    context_list = [doc.page_content for doc in reranked_docs]\n",
    "    all_contexts.append(context_list)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_list)\n",
    "    # Create proper prompt format for the LLM\n",
    "    prompt = f\"\"\"Answer based ONLY on:\n",
    "    Context: {context_text}\n",
    "    Question: {query}\n",
    "    Concise medical answer (max 200 words):\"\"\"\n",
    "\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "   \n",
    "    response = await llm_wrapper.generate([messages])\n",
    "\n",
    "    response = await llm_wrapper.generate(prompt)\n",
    "    generated_response = response.generations[0][0].text\n",
    "    generated_responses.append(generated_response.strip())\n",
    "\n",
    "# Step 5: Prepare data in HuggingFace format\n",
    "test_data = {\n",
    "    \"question\": queries,\n",
    "    \"contexts\": all_contexts,\n",
    "    \"response\": generated_responses,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# Step 6: Evaluate\n",
    "print(\"\\n evaluating Faithfulness...\")\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness],\n",
    "    llm=llm_wrapper\n",
    ")\n",
    "\n",
    "#  Step 7: Show Results\n",
    "print(\"\\n generating results:\")\n",
    "print(\"=\" * 60)\n",
    "for i, (q, score) in enumerate(zip(queries, results[\"faithfulness\"])):\n",
    "    print(f\"\\nquestion {i+1}: {q[:60]}...\")\n",
    "    print(f\"• Faithfulness: {score:.2f}/1.0\")\n",
    "    print(f\"• the number of documents used: {len(all_contexts[i])}\")\n",
    "    print(\"• context summary:\")\n",
    "    for j, ctx in enumerate(all_contexts[i][:2]):\n",
    "        print(f\"  - [{j+1}] {ctx[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKRr9VEqp1cp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c654b0a08bd84127bc98370c84eb7890",
      "12be42a4d1fc4d70bdffe9aeeb11a6e0",
      "c6e84ab607824e0dab8c5c3dfc693b10",
      "325845f188424863b0871ee46ba3fda6",
      "5c2cd81bcf1245078b6269b97a632138",
      "f62e6177ee174206b456dea927884ae8",
      "54e42f1ad2944426be6612d834021dce",
      "83f0c212c6474dfda4005c08b9f6e560",
      "c10937f4dd934c24bf8117e036bc76bb",
      "88775687800b46e0b1789f7ad1dc15e3",
      "38578f60b45e452b9419c6728d883355"
     ]
    },
    "id": "5jpIV0PCp1fx",
    "outputId": "b58d9bb2-57a7-48c0-d2e3-28edff38e56a"
   },
   "outputs": [],
   "source": [
    "# Step 0: Installation (skip if already installed)\n",
    "!pip install -U ragas langchain-openai datasets\n",
    "\n",
    "# Step 1: Imports & API Key\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.metrics import faithfulness, answer_relevancy  # Added answer_relevancy\n",
    "from langchain_core.messages import HumanMessage\n",
    "from ragas.metrics import ResponseRelevancy  # Import ResponseRelevancy\n",
    "\n",
    "queries = [\n",
    "   \"How does sonodynamic therapy (SDT) differ from conventional antibiotics in terms of combating multidrug-resistant bacterial infections?\",\n",
    "   \"Does zebrafish possess an ortholog or paralog of mammalian calprotectin that exhibits antimicrobial activity and can activate inflammation via Toll-like receptor 4?\",\n",
    "   \"What is the current understanding of gray matter alterations in patients with vestibular migraine?\",\n",
    "   \"How do nickel clusters improve the hydrogen evolution reaction (HER) and what potential applications does this have in renewable energy conversion?\",\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "  \" Sonodynamic therapy (SDT) leverages the generation of reactive oxygen species to inflict multifaceted damage on bacterial cells, reducing the likelihood of developing drug resistance, unlike conventional antibiotics.\",\n",
    "   \"No, zebrafish do not have an ortholog of either mammalian S100A8 or S100A9, and none of the identified zebrafish s100 proteins exhibited antimicrobial activity comparable to mammalian calprotectin. Additionally, none of the zebrafish proteins activated inflammation via Toll-like receptor 4, suggesting that similar proteins have not convergently evolved analogous functions.\",\n",
    "   \"The current understanding of gray matter alterations in patients with vestibular migraine remains lacking, despite the growing amount of neuroimaging data in recent decades.\",\n",
    "   \"clusters considerably improve the hydrogen evolution reaction (HER), indicating their promise for renewable energy conversion.\"\n",
    "]\n",
    "\n",
    "# ✅ Step 3: Initialize LLM and embedding wrappers for RAGAS\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_wrapper = LangchainLLMWrapper(llm)\n",
    "embedding_wrapper = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# Initialize ResponseRelevancy scorer\n",
    "response_relevancy = ResponseRelevancy(\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper\n",
    ")\n",
    "\n",
    "# ✅ Step 4: Retrieval + Generation\n",
    "all_contexts = []\n",
    "generated_responses = []\n",
    "relevancy_scores = []\n",
    "\n",
    "for query in queries:\n",
    "\n",
    "\n",
    "    retrieved_docs = db.as_retriever(search_kwargs={\"k\": 15}).get_relevant_documents(query)\n",
    "    reranked_docs = rerank_bge(query, retrieved_docs, top_k=4)\n",
    "\n",
    "    context_list = [doc.page_content for doc in reranked_docs]\n",
    "    all_contexts.append(context_list)\n",
    "\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_list)\n",
    "    prompt = f\"\"\"Answer based ONLY on:\n",
    "    Context: {context_text}\n",
    "    Question: {query}\n",
    "    Concise medical answer (max 150 words):\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = await llm.agenerate([messages])\n",
    "    generated_response = response.generations[0][0].text.strip()\n",
    "    generated_responses.append(generated_response)\n",
    "\n",
    "    # Calculate response relevancy score\n",
    "    sample = {\n",
    "        \"user_input\": query,\n",
    "        \"response\": generated_response,\n",
    "        \"retrieved_contexts\": context_list\n",
    "    }\n",
    "    relevancy_score = await response_relevancy.ascore(sample)\n",
    "    relevancy_scores.append(relevancy_score)\n",
    "\n",
    "# ✅ Step 5: Prepare data in HuggingFace format\n",
    "test_data = {\n",
    "    \"question\": queries,\n",
    "    \"contexts\": all_contexts,\n",
    "    \"response\": generated_responses,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "# ✅ Step 6: Evaluate with multiple metrics\n",
    "print(\"\\n Evaluating Metrics...\")\n",
    "results = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=[faithfulness, answer_relevancy],  # Added answer_relevancy\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper\n",
    ")\n",
    "\n",
    "# ✅ Step 7: Show Results\n",
    "print(\"\\n Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "for i, (q, faith_score, rel_score, manual_rel_score) in enumerate(zip(\n",
    "    queries,\n",
    "    results[\"faithfulness\"],\n",
    "    results[\"answer_relevancy\"],\n",
    "    relevancy_scores\n",
    ")):\n",
    "    print(f\"\\n Question {i+1}: {q[:60]}...\")\n",
    "    print(f\"• Faithfulness: {faith_score:.2f}/1.0\")\n",
    "    print(f\"• Answer Relevancy (RAGAS): {rel_score:.2f}/1.0\")\n",
    "    # print(f\"• Response Relevancy (Direct): {manual_rel_score:.2f}/1.0\")\n",
    "    print(f\"• Documents used: {len(all_contexts[i])}\")\n",
    "    print(\"• Context summary:\")\n",
    "    for j, ctx in enumerate(all_contexts[i][:2]):\n",
    "        print(f\"  - [{j+1}] {ctx[:100]}...\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n💡 Interpretation:\")\n",
    "print(\"- Faithfulness: 1.0 = fully consistent with documents\")\n",
    "print(\"- Relevancy: 1.0 = perfectly addresses the question\")\n",
    "print(\"- <0.5 scores indicate potential issues\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enhanced Context Analysis\n",
    "print(\"\\n Deep Context Analysis\")\n",
    "print(\"=\" * 80)\n",
    "for i, (q, contexts) in enumerate(zip(queries, all_contexts)):\n",
    "    print(f\"\\n Question {i+1}: {q}\")\n",
    "    print(f\"\\n Generated Answer ({len(generated_responses[i])} chars):\")\n",
    "    print(f\"{generated_responses[i]}\")\n",
    "\n",
    "    print(f\"\\n Retrieved Contexts ({len(contexts)}):\")\n",
    "    for j, ctx in enumerate(contexts):\n",
    "        print(f\"\\n  - Context {j+1} ({len(ctx)} chars):\")\n",
    "        print(ctx if len(ctx) <= 800 else ctx[:800] + \"... [truncated]\")\n",
    "\n",
    "    print(f\"\\n⚡ Scores:\")\n",
    "    print(f\"   Faithfulness: {results['faithfulness'][i]:.2f} | Answer Relevancy: {results['answer_relevancy'][i]:.2f} | Response Relevancy: {relevancy_scores[i]:.2f}\")\n",
    "    print(f\"\\n Problem Diagnosis:\")\n",
    "    if \"Sample context about\" in contexts[0]:\n",
    "        print(\"    Critical: Using placeholder contexts instead of real documents\")\n",
    "    elif results['faithfulness'][i] < 0.5:\n",
    "        print(\"    Low Faithfulness: Answer not grounded in contexts\")\n",
    "    elif results['answer_relevancy'][i] < 0.5:\n",
    "        print(\"    Low Relevancy: Answer doesn't match question intent\")\n",
    "    else:\n",
    "        print(\"    Good scores (but verify contexts are real)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqHz34Syp1iC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuJrYMyBp1k6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07yx4KHyRjSk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
