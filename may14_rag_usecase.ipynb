{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AELZHL_EseiF"
   },
   "outputs": [],
   "source": [
    "# Uncomment the following block to install required libraries\n",
    "# \"\"\"\n",
    "# !pip install langchain chromadb sentence-transformers\n",
    "# !pip install  openai tiktoken\n",
    "# !pip install jq\n",
    "# !pip install faiss\n",
    "# !pip install pymilvus\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-03ZQ8Q6RCul"
   },
   "outputs": [],
   "source": [
    "\n",
    "!git config --global user.email \"xz3761@nyu.edu\"\n",
    "!git config --global user.name \"skylarzhao1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z73unqPJrQiW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NidABYGVWfGF",
    "outputId": "bb8ebae9-4379-42a6-9e7d-fba27d3cc070"
   },
   "outputs": [],
   "source": [
    "!git -C ColBERT/ pull || git clone https://github.com/stanford-futuredata/ColBERT.git\n",
    "import sys; sys.path.insert(0, 'ColBERT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Qtk6-n0Wksd",
    "outputId": "81c4d3d1-c770-4a94-cb90-3d00cbd6a65f"
   },
   "outputs": [],
   "source": [
    "try: # When on google Colab, let's install all dependencies with pip.\n",
    "    import google.colab\n",
    "    !pip install -U pip\n",
    "    !pip install -e ColBERT/['faiss-gpu','torch']\n",
    "except Exception:\n",
    "  import sys; sys.path.insert(0, 'ColBERT/')\n",
    "  try:\n",
    "    from colbert import Indexer, Searcher\n",
    "  except Exception:\n",
    "    print(\"If you're running outside Colab, please make sure you install ColBERT in conda following the instructions in our README. You can also install (as above) with pip but it may install slower or less stable faiss or torch dependencies. Conda is recommended.\")\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hj_vDYRdWpiT",
    "outputId": "022c9308-529e-478a-b82c-c7591efa787e"
   },
   "outputs": [],
   "source": [
    "!pip install ujson\n",
    "import colbert\n",
    "from colbert import Indexer, Searcher\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert.data import Queries, Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFDFjpkqWk0g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72QfglKmWk34"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1s9WIqTWk6s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhRylV-cEr7S",
    "outputId": "e227b1db-7a3a-47f3-b76a-570380992e17"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index-core transformers torch\n",
    "!pip install llama-index-postprocessor-colbert-rerank\n",
    "!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3isaDjNjFLmU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkRGq6xQANRj"
   },
   "outputs": [],
   "source": [
    "!pip install -qU bitsandbytes transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RZYc6m2xAO1N"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0hMy18ruhrt",
    "outputId": "43aab34e-cb56-4cdc-ee76-e0bfa6da6e0f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWae0mSNFNcB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjuK-gA9seiG"
   },
   "source": [
    "- Setting the API key of HuggingFace to load the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nX2asmzHseiG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']='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'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHouMxxBseiG"
   },
   "source": [
    "* Load the PubMed articles from the JSON file. To prepare the JSON file, please refer to the script `download_pubmed.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHWruWXiseiG",
    "outputId": "b1294c22-f1c5-4ca3-e529-dcefec2d785f"
   },
   "outputs": [],
   "source": [
    "!pip install -U langchain langchain-community\n",
    "!pip install jq\n",
    "#加载 JSON 文件里的 PubMed 文章摘要，并封装成 LangChain 能处理的 Document 格式，方便后续做 chunking、embedding 和 RAG 检索\n",
    "from langchain.document_loaders import JSONLoader\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    # Define the metadata extraction function.\n",
    "    metadata[\"year\"] = record.get(\"pub_date\").get('year')\n",
    "    metadata[\"month\"] = record.get(\"pub_date\").get('month')\n",
    "    metadata[\"day\"] = record.get(\"pub_date\").get('day')\n",
    "    metadata[\"title\"] = record.get(\"article_title\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='/content/drive/MyDrive/finalproject/pubmed_article.json',\n",
    "    jq_schema='.[]',\n",
    "    content_key='article_abstract',\n",
    "    metadata_func=metadata_func)\n",
    "data = loader.load()\n",
    "print(f\"{len(data)} pubmed articles are loaded!\")\n",
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVAZ8sKtW6AA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76voz0KMseiH"
   },
   "source": [
    "- Chunk abstracts into small text passages for efficient retrieval and LLM context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaag0-jRseiH",
    "outputId": "8f46bd48-a6cc-4d2a-abd3-743e15b59678"
   },
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "#只对page content 做chunking\n",
    "from langchain.text_splitter import TokenTextSplitter,CharacterTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=64)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"{len(data)} pubmed articles are converted to {len(chunks)} text fragments!\")\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaF6-ijjseiH"
   },
   "source": [
    "- Load the embedding model. The following code defines two options for loading the model:\n",
    "    - **Option a:** Using SentenceTransformerEmbeddings framework to load their most performing model `all-mpnet-base-v2`\n",
    "    - **Option b:** Using HuggingFaceEmbeddings hub to load the popular model `e5-large-unsupervised`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMZeOqweseiI"
   },
   "outputs": [],
   "source": [
    "# Option a: using all-mpnet from SentenceTransformer\n",
    "# from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# embeddings = SentenceTransformerEmbeddings(model_name=\"all-mpnet-base-v2\")\n",
    "\n",
    "# # Option b: using e5-large-unspupervised from huggingface\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# modelPath = \"intfloat/e5-large-unsupervised\"\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#   model_name = modelPath,\n",
    "#   model_kwargs = {'device':'cuda'},\n",
    "#   encode_kwargs={'normalize_embeddings':False}\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"intfloat/e5-large-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GVRyLeI2Kvp",
    "outputId": "f5bc45ac-3b1c-4c62-a481-21a9ac78b3f0"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-gpu-cu12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WVUzXU_sseiI",
    "outputId": "029bbc24-67b7-4443-9fb3-4de1ba7ba2c8"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Option a: Using chroma database\n",
    "from langchain.vectorstores import Chroma\n",
    "# db = Chroma.from_documents(chunks, embeddings)\n",
    "# 用正确的 1024 维模型加载\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "\n",
    "'''\n",
    "# Option b: Using Milvus database\n",
    "# To run the following code, you should have a milvus instance up and running\n",
    "# Follow the instructions in the following the link: https://milvus.io/docs/install_standalone-docker.md\n",
    "from langchain.vectorstores import Milvus\n",
    "db = Milvus.from_documents(\n",
    "    chunks,\n",
    "    embeddings,\n",
    "    connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"},\n",
    ")\n",
    "'''\n",
    "\n",
    "# # Using faiss index\n",
    "# from langchain.vectorstores import FAISS\n",
    "# db = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDKXTfAlseiI"
   },
   "source": [
    "- Load pre-trained Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a7c672ec53264bd1a02fa5cb1e108501",
      "f119211710c740a5a4894e6156fd9de4",
      "2a174da5882e4560ae680d948c0cf3d1",
      "99196438703f47eeb06da19df029c23f",
      "3193c11c05544d3a99725c98f44b7ce4",
      "dc7ff6c23ba5400f91ac9065e3726c09",
      "f67074543ebf40dd9c44a3f79ddbe8da",
      "f620f705156a4dae814786cbceeeb9b4",
      "3b6bb820f7f442e1a2d5578b3ca4facb",
      "3ee026a0fb334c00addf14658d078d12",
      "4b66c9d478df4e53a5c4fd04c870d434"
     ]
    },
    "id": "AY9m6suKy0W4",
    "outputId": "702fd6ae-73a1-4952-ee92-084ef727dad4"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# First uninstall existing versions\n",
    "!pip uninstall -y bitsandbytes transformers accelerate\n",
    "\n",
    "# Install with CUDA 11.x support (works for most systems)\n",
    "!pip install -U bitsandbytes>=0.41.1\n",
    "!pip install -U transformers accelerate\n",
    "\n",
    "# Verify installation\n",
    "import bitsandbytes\n",
    "print(f\"bitsandbytes version: {bitsandbytes.__version__}\")\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model (Mistral-7B)\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,  # Add this for 4-bit\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Create pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.1,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# LangChain wrapper\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00XdR-8VyzQ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bj52OgdhseiI"
   },
   "source": [
    "- Define the RAG pipeline using LangChain. The LLM's answer highly depends on the prompt template, that's why we tested three different prompts. The one giving the best answer as PROMPT2.\n",
    "\n",
    "#TODO: Add explanation about the three prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQRzlviGseiI",
    "outputId": "e287b873-9d53-4867-ed8f-5ad9ffb8dcbd"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import time\n",
    "\n",
    "# PROMPT 1\n",
    "PROMPT_TEMPLATE_1 = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "You are allowed to rephrase the answer based on the context.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "PROMPT1 = PromptTemplate.from_template(PROMPT_TEMPLATE_1)\n",
    "\n",
    "# # PROMPT 2\n",
    "# PROMPT_TEMPLATE_2=\"Your are a medical assistant for question-answering tasks. Answer the Question using the provided Contex only. Your answer should be in your own words and be no longer than 128 words. \\n\\n Context: {context} \\n\\n Question: {question} \\n\\n Answer:\"\n",
    "# PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "# 1. 修正Prompt\n",
    "PROMPT_TEMPLATE_2 = \"\"\"Answer based ONLY on:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Concise medical answer (max 100 words):\"\"\"\n",
    "PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "\n",
    "# # 3. 执行查询\n",
    "# start_time = time.time()\n",
    "# result = qa_chain.invoke({\"query\": \"Alzheimer's treatments\"})\n",
    "# print(f\"Time: {time.time() - start_time:.2f}s\")\n",
    "# print(result[\"result\"])\n",
    "# PROMPT 3\n",
    "from langchain import hub\n",
    "PROMPT3 = hub.pull(\"rlm/rag-prompt\", api_url=\"https://api.hub.langchain.com\")\n",
    "\n",
    "# RAG pipeline\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=db.as_retriever(k=4),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT2},\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN2m0ca6seiI"
   },
   "source": [
    "- Run one sample query `\"What are the safest cryopreservation methods?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OyT1eX5seiI",
    "outputId": "116144a9-f23d-42aa-f9ad-c811d0a5ff9a"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# query = \"What are the safest cryopreservation methods?\"\n",
    "query=\"What are the recent advancements in the treatment of Alzheimer’s disease?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "print(f\"\\n--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1b_-KGDOEttj",
    "outputId": "29a27f6c-48a3-4911-cab8-096ff59d9361"
   },
   "outputs": [],
   "source": [
    "retrieved_docs = result[\"source_documents\"]\n",
    "\n",
    "print(f\"\\n 共检索到 {len(retrieved_docs)} 个文献段落（chunk）：\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\" Chunk {i+1}:\")\n",
    "    print(f\" Title: {doc.metadata.get('title')}\")\n",
    "    print(f\" Content Preview: {doc.page_content[:200]}...\")  # 只显示前200个字\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsttziPOseiJ",
    "outputId": "a515cbc9-2df5-4386-c695-08d5f84e874b"
   },
   "outputs": [],
   "source": [
    "print(result['result'].strip())\n",
    "titles = ['\\t-'+doc.metadata['title'] for doc in result['source_documents']]\n",
    "print(\"\\n\\nThe provided answer is based on the following PubMed articles:\\t\")\n",
    "print(\"\\n\".join(set(titles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7nBdB9hseiJ"
   },
   "source": [
    "- Get the answer to the sample query from the LLM only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cT2GflVwseiJ",
    "outputId": "dda66892-75d8-43cf-b542-7908ac9e5750"
   },
   "outputs": [],
   "source": [
    "# Define the langchain pipeline for llm only\n",
    "from langchain.prompts import PromptTemplate\n",
    "PROMPT_TEMPLATE =\"\"\"Answer the given Question only. Your answer should be in your own words and be no longer than 100 words. \\n\\n Question: {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "llm_chain = PROMPT | llm\n",
    "start_time = time.time()\n",
    "result = llm_chain.invoke({\"question\": query})\n",
    "print(f\"\\n--- {time.time() - start_time} seconds ---\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524,
     "referenced_widgets": [
      "ceeae6e0f371475c91deb24d370772b2",
      "c5d329e6ee90432e9aad09c2940e9429",
      "8248f0af522f4ef4a67c6a5aa5cddbfe",
      "2918f75d589e493d938a0ed7d8edd3cb",
      "d95347e194ce4eceb91e2b95d6d6ed07",
      "b9aebf83ead544fda7d362563e702e2f",
      "e7da8633f98546458a978069df9d4744",
      "240cdaeaf4df42b0a147c2422c31097b",
      "41072f9f827547e0b1f0d2afda16428e",
      "ebff9a72eb8845d6bf577b1e09c7deff",
      "8040c44e031744e68b4fd0cb53746c93"
     ]
    },
    "id": "fPpo9VCtErN1",
    "outputId": "5b193d51-0c64-41b5-c03d-fec7af05da6b"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.cuda.is_available())\n",
    "\n",
    "\n",
    "# #Indexing\n",
    "\n",
    "\n",
    "# nbits = 2   # encode each dimension with 2 bits\n",
    "# doc_maxlen = 128 # truncate passages at 300 tokens\n",
    "# max_id = 20\n",
    "# checkpoint = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "\n",
    "# collection = [chunk.page_content for chunk in chunks[:max_id]]  # 限制了 max_id=2400，所以截取前 2400 条\n",
    "\n",
    "\n",
    "# index_name = 'pubmed_index_small'\n",
    "\n",
    "# with Run().context(RunConfig(nranks=1, experiment=\"/content/drive/MyDrive/finalproject/exp_pubmed\")):\n",
    "\n",
    "#     config = ColBERTConfig(doc_maxlen=128, nbits=2, kmeans_niters=2)\n",
    "#     indexer = Indexer(checkpoint=checkpoint, config=config)\n",
    "#     indexer.index(name=index_name, collection=collection, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1X3G6lVoILFZ",
    "outputId": "fc6c0a2d-5ca6-4c21-ad11-93c7fe46db2a"
   },
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers\n",
    "from sentence_transformers import CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "b9d3ed0dbc5c40668071abf9088b790e",
      "23c75540adf145599f9c7da320e56df9",
      "0a6971956d784796b276348850a07898",
      "10dd04d813e94769864a4c3a4ee3a471",
      "a59b0a399cbb441e9e95d194ff93c035",
      "322b27f57e8c42b8b6b75aad16457227",
      "661d4c8bf2044d8ba67282606ab742a5",
      "4a492dba9b844c4a996160f3032f6419",
      "ea2f65a944f544d09291396029ec8ae7",
      "d4ae68c341214a9b9392a4ccc43a3371",
      "4c55eaa5f9a645469d5cda1921b9fcdd",
      "80b3cf78c98b4ca5b9b49155339c75a6",
      "0c4a9ad1f5564e30b91f4982a1b1bd80",
      "7e3eaacbcc0c45378d87671262fdb846",
      "9730549f4b244d269a3aa8d8fc97b39d",
      "b80af5a8bcb34a42981e8579657482d5",
      "fc39803636004d7787d9473e4c7e40ca",
      "2a81d96bccf04b459d7f32ef218b934c",
      "de5167f02f004556aec57ba175ba3454",
      "69055bfa867b4839bb8d45df718968a3",
      "3c42760742664ac197c3e63b4229db0f",
      "2c383b52bd8f44b8853c5e806210c293",
      "a6553213ef5748af81468539e76ee902",
      "456419e82b5a435f82306729f3cc0df2",
      "ed3a0b5891d84ac6a7293f3f2642fb06",
      "462e8086ccc44fa8ad00e663409b9ee3",
      "c8ab3e68080847d5bcc566150c368f9d",
      "b725a245758a472faabc137f6437342b",
      "240e5fdd44ee43b9843c570f8651bd70",
      "f4093afb8f05414a872ff24d0418606b",
      "420a22cbfe2441ada3e0138b0dbc2809",
      "ee4c5034970a436bb35fdf94de88e941",
      "66180ed8e4af46d0992c82933b8e0bdf",
      "1e73897c9b2245e0874285bf77b87f02",
      "45a11038e76244169cbe254fca88a680",
      "2df4be1ee0a440b187fdeb751e46c93f",
      "5ea706470bb64ad590e9f68b3adbf006",
      "c959e692ea214e76adc0d92b930bab96",
      "7717b7e1c10242d093deedab884029b6",
      "e72009b3764148b5b8152ffb236e6c03",
      "d41f2935b4a440488b786bcdadf074b6",
      "8d3ea317c9a14902a215d683198cf55d",
      "1818bec6cf8e44f8a55373ef35039af1",
      "00978f9436474edca12645b76fc89ace",
      "7ad9b6f793f44a2382a9d11297bae1bb",
      "469c8979f2af48aaa6328ddc7a61e4b2",
      "759f1d6fe2e8491896b00a13c669c789",
      "f9956c7dbce145ec96466e7efece347f",
      "f1d3d4adc5c54fb794addce36609d810",
      "87b58861f37346029062260fa82821ce",
      "cbbfe37ec7f045edb31d9b1e51b26b54",
      "0ad9edcf109144349cf77fa7d6e0d48e",
      "527a946d042b45c3a1017f2d983f0aab",
      "d82eab1db7394289a1f52803d1a849ae",
      "3c221bd98cca4e94a4f5771c988ec423",
      "f8c0dedf482d44dcae7f573b218d2f52",
      "fee0a4fdabbd4eff81bcd215a75d74a5",
      "8da5ffb6ef1e4b72a721e1793726d79f",
      "55cb251237854ad293761d5e92e51b7d",
      "27edf90283744401b490061550dd62e0",
      "2134ba7c13b34bc2b292258de674b61a",
      "374a3b9fa8594afe96d205bbe398ad09",
      "89c59b66ad4040d385818e216b608042",
      "2585daf14ca24fd385a1ddb8dec10cb8",
      "22be30e38c74494393e29e10367d2b25",
      "85260532b0a5430d8919f7ac84bc21b3"
     ]
    },
    "id": "QMMvt5Q0ILHx",
    "outputId": "58bd9e3c-6b5d-48bd-a8c6-42f5776c6388"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "# 加载 BGE Reranker 模型\n",
    "rerank_model_name = \"BAAI/bge-reranker-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  #\n",
    "def rerank_bge(query, documents, top_k=3):\n",
    "    scores = []\n",
    "    for doc in documents:\n",
    "        inputs = tokenizer(\n",
    "            query, doc.page_content,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            score = logits.squeeze().item()  #\n",
    "\n",
    "        scores.append((doc, score))\n",
    "\n",
    "    reranked = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    for i, (doc, score) in enumerate(reranked):\n",
    "        print(f\"Reranked #{i+1} | Score: {score:.4f}\")\n",
    "        print(f\"Title: {doc.metadata.get('title')}\")\n",
    "        print(f\"Content: {doc.page_content[:300]}...\\n\")\n",
    "    return [doc for doc, score in reranked]\n",
    "\n",
    "\n",
    "# query = \"What are the recent advancements in the treatment of Alzheimer’s disease?\"\n",
    "# retrieved_docs = db.as_retriever(k=8).get_relevant_documents(query)\n",
    "\n",
    "# # rerank\n",
    "# reranked_docs = rerank_bge(query, retrieved_docs, top_k=7)\n",
    "\n",
    "\n",
    "# print(f\"\\n Query: {query}\")\n",
    "# print(f\" Top {len(reranked_docs)} reranked results:\\n\")\n",
    "# for i, doc in enumerate(reranked_docs):\n",
    "#     print(f\"Reranked #{i+1}:\")\n",
    "#     print(f\"Title: {doc.metadata.get('title')}\")\n",
    "#     print(f\"Content: {doc.page_content[:300]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-g2onRwuILJ1",
    "outputId": "3e8b90f8-be0b-4e3e-8ff7-b12e407dc556"
   },
   "outputs": [],
   "source": [
    "query = \"What are the recent advancements in the treatment of Alzheimer’s disease?\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE_2 = \"\"\"Answer based ONLY on:\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Concise medical answer:\"\"\"\n",
    "PROMPT2 = PromptTemplate.from_template(PROMPT_TEMPLATE_2)\n",
    "\n",
    "# Step 1: 检索\n",
    "retrieved_docs = db.as_retriever(k=7).get_relevant_documents(query)\n",
    "\n",
    "# Step 2: rerank\n",
    "reranked_docs = rerank_bge(query, retrieved_docs, top_k=4)\n",
    "\n",
    "# Step 3: 拼接上下文\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "\n",
    "# Step 4: 构造 Prompt\n",
    "prompt = PROMPT2.format(context=context, question=query)\n",
    "\n",
    "# Step 5: LLM 推理\n",
    "response = llm.invoke(prompt)\n",
    "print(\" Final Answer:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Pp5D44RCBpK",
    "outputId": "28f45fb2-a14f-4ebb-b9bd-d688d084272b"
   },
   "outputs": [],
   "source": [
    "!pip install ragas\n",
    "\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import Faithfulness\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "Q26EDOk9ILMM",
    "outputId": "03a1ef77-819c-40d1-b417-e86b536d223c"
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "print(type(result))\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reranked_docs = rerank_bge(query, retrieved_docs, top_k=3)\n",
    "\n",
    "# 获取 LLM 的回答（就是你原来的 result['result']）\n",
    "answer = result\n",
    "\n",
    "# 把文献内容提取出来（只要 text 就行）\n",
    "retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "\n",
    "# 构建 ragas 的 Sample 对象\n",
    "sample = SingleTurnSample(\n",
    "    user_input=query,\n",
    "    response=answer,\n",
    "    retrieved_contexts=retrieved_contexts\n",
    ")\n",
    "\n",
    "scorer = FaithfulnesswithHHEM(\n",
    "    device=\"cuda:0\",  # 使用GPU加速\n",
    "    batch_size=8      # 调整批量大小\n",
    ")\n",
    "# 运行评估（必须用异步）\n",
    "async def evaluate_faithfulness():\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(f\"Faithfulness Score: {score:.2f}\")\n",
    "\n",
    "await evaluate_faithfulness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "8r7VixrbIHOV",
    "outputId": "4d68302a-937d-4647-fa85-b3edb1f1ff88"
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "print(type(result))\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reranked_docs = rerank_bge(query, retrieved_docs, top_k=3)\n",
    "\n",
    "# 获取 LLM 的回答（就是你原来的 result['result']）\n",
    "answer = result\n",
    "\n",
    "# 把文献内容提取出来（只要 text 就行）\n",
    "retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "# 构建样本（替换为您的数据）\n",
    "# 构建 ragas 的 Sample 对象\n",
    "sample = SingleTurnSample(\n",
    "    user_input=query,\n",
    "    response=answer,\n",
    "    retrieved_contexts=retrieved_contexts\n",
    ")\n",
    "\n",
    "# 关键修改：直接初始化，不传LLM参数\n",
    "scorer = FaithfulnesswithHHEM(device=\"cuda:0\")  # 移除了batch_size参数\n",
    "\n",
    "async def evaluate_faithfulness():\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(f\"Faithfulness Score: {score:.2f}\")\n",
    "\n",
    "await evaluate_faithfulness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l61xCCdtIqn7",
    "outputId": "ac172268-17d2-43f6-b2ec-0536500cbacb"
   },
   "outputs": [],
   "source": [
    "!pip install ragas nest_asyncio\n",
    "\n",
    "from ragas.metrics import FaithfulnesswithHHEM\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reranked_docs = rerank_bge(query, retrieved_docs, top_k=3)\n",
    "\n",
    "# 获取 LLM 的回答（就是你原来的 result['result']）\n",
    "answer = result\n",
    "\n",
    "# 把文献内容提取出来（只要 text 就行）\n",
    "retrieved_contexts = [doc.page_content for doc in reranked_docs]\n",
    "# 构建样本（替换为您的数据）\n",
    "# 构建 ragas 的 Sample 对象\n",
    "sample = SingleTurnSample(\n",
    "    user_input=query,\n",
    "    response=answer,\n",
    "    retrieved_contexts=retrieved_contexts\n",
    ")\n",
    "\n",
    "# 关键修复步骤\n",
    "scorer = FaithfulnesswithHHEM()\n",
    "setattr(scorer, 'llm', 'hhem-v2')  # 强制注入模型标识\n",
    "\n",
    "async def evaluate():\n",
    "    score = await scorer.single_turn_ascore(sample)\n",
    "    print(f\"Faithfulness Score: {score:.2f}\")\n",
    "\n",
    "await evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "s71oqXpFIqtO",
    "outputId": "f74c3f4f-be6f-4f61-fdff-9f014fc7a7d9"
   },
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AspectCritic\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": \"summarise given text\\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.\",\n",
    "    \"response\": \"The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.\",\n",
    "}\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "metric = AspectCritic(name=\"summary_accuracy\",llm=evaluator_llm, definition=\"Verify if the summary is accurate.\")\n",
    "await metric.single_turn_ascore(SingleTurnSample(**test_data))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
